# Taco Snake Benchmark - Results Comparison

**Benchmark Date**: [Start Date] - [End Date]
**Evaluator**: [Your Name]
**Benchmark Version**: 1.0

---

## ğŸ“Š Overall Results Summary

| Model | Tool | Final Score | Status | Features | Quality |
|-------|------|-------------|--------|----------|---------|
| [Model 1] | [Tool] | __% | âœ…/âŒ | __/15 | __/10 |
| [Model 2] | [Tool] | __% | âœ…/âŒ | __/15 | __/10 |
| [Model 3] | [Tool] | __% | âœ…/âŒ | __/15 | __/10 |
| [Model 4] | [Tool] | __% | âœ…/âŒ | __/15 | __/10 |

> **Note**: Time and token metrics are NOT included in comparisons as they vary significantly between local (hardware-dependent) and cloud models.

---

## ğŸ† Rankings

### By Final Score
1. [Model Name] - [Tool] - __%
2. [Model Name] - [Tool] - __%
3. [Model Name] - [Tool] - __%
4. [Model Name] - [Tool] - __%

### By Feature Completeness
1. [Model Name] - __/15 features
2. [Model Name] - __/15 features
3. [Model Name] - __/15 features
4. [Model Name] - __/15 features

### By Code Quality
1. [Model Name] - __/10
2. [Model Name] - __/10
3. [Model Name] - __/10
4. [Model Name] - __/10

---

## ğŸ“ˆ Detailed Score Breakdown

### [Model 1 Name] - [Tool Name]

**Final Score**: __%
**Status**: âœ… PASS / âŒ FAIL

| Category | Score | Weight | Weighted |
|----------|-------|--------|----------|
| Functional | __% | 40% | __% |
| Visual | __% | 20% | __% |
| Technical | __% | 20% | __% |
| Quality | __% | 20% | __% |

**Strengths**:
-

**Weaknesses**:
-

**Notable Issues**:
-

---

### [Model 2 Name] - [Tool Name]

**Final Score**: __%
**Status**: âœ… PASS / âŒ FAIL

| Category | Score | Weight | Weighted |
|----------|-------|--------|----------|
| Functional | __% | 40% | __% |
| Visual | __% | 20% | __% |
| Technical | __% | 20% | __% |
| Quality | __% | 20% | __% |

**Strengths**:
-

**Weaknesses**:
-

**Notable Issues**:
-

---

### [Model 3 Name] - [Tool Name]

**Final Score**: __%
**Status**: âœ… PASS / âŒ FAIL

| Category | Score | Weight | Weighted |
|----------|-------|--------|----------|
| Functional | __% | 40% | __% |
| Visual | __% | 20% | __% |
| Technical | __% | 20% | __% |
| Quality | __% | 20% | __% |

**Strengths**:
-

**Weaknesses**:
-

**Notable Issues**:
-

---

### [Model 4 Name] - [Tool Name]

**Final Score**: __%
**Status**: âœ… PASS / âŒ FAIL

| Category | Score | Weight | Weighted |
|----------|-------|--------|----------|
| Functional | __% | 40% | __% |
| Visual | __% | 20% | __% |
| Technical | __% | 20% | __% |
| Quality | __% | 20% | __% |

**Strengths**:
-

**Weaknesses**:
-

**Notable Issues**:
-

---

## ğŸ” Feature Implementation Comparison

| Feature | [Model 1] | [Model 2] | [Model 3] | [Model 4] |
|---------|-----------|-----------|-----------|-----------|
| 4-direction movement | âœ…/âŒ | âœ…/âŒ | âœ…/âŒ | âœ…/âŒ |
| No reverse direction | âœ…/âŒ | âœ…/âŒ | âœ…/âŒ | âœ…/âŒ |
| Starts with 3 segments | âœ…/âŒ | âœ…/âŒ | âœ…/âŒ | âœ…/âŒ |
| Grows when eating | âœ…/âŒ | âœ…/âŒ | âœ…/âŒ | âœ…/âŒ |
| One taco at a time | âœ…/âŒ | âœ…/âŒ | âœ…/âŒ | âœ…/âŒ |
| Random taco spawn | âœ…/âŒ | âœ…/âŒ | âœ…/âŒ | âœ…/âŒ |
| 10 points per taco | âœ…/âŒ | âœ…/âŒ | âœ…/âŒ | âœ…/âŒ |
| Score display | âœ…/âŒ | âœ…/âŒ | âœ…/âŒ | âœ…/âŒ |
| Wall collision | âœ…/âŒ | âœ…/âŒ | âœ…/âŒ | âœ…/âŒ |
| Self collision | âœ…/âŒ | âœ…/âŒ | âœ…/âŒ | âœ…/âŒ |
| Game Over message | âœ…/âŒ | âœ…/âŒ | âœ…/âŒ | âœ…/âŒ |
| Final score shown | âœ…/âŒ | âœ…/âŒ | âœ…/âŒ | âœ…/âŒ |
| Restart works | âœ…/âŒ | âœ…/âŒ | âœ…/âŒ | âœ…/âŒ |
| Taco visually clear | âœ…/âŒ | âœ…/âŒ | âœ…/âŒ | âœ…/âŒ |
| Snake segmented | âœ…/âŒ | âœ…/âŒ | âœ…/âŒ | âœ…/âŒ |

---

## ğŸ’¡ Key Insights

### What Worked Well Across Models
-

### Common Failures
-

### Tool Comparison (Claude Code vs Opencode)
-

### Model Comparison (Claude vs Qwen3 vs Others)
-

### Unexpected Results
-

---

## ğŸ“ Conclusions

### Best Overall Implementation
**Winner**: [Model Name] - [Tool Name]
**Reason**:

### Best for Specific Categories

- **Most Feature-Complete**: [Model Name]
- **Highest Code Quality**: [Model Name]
- **Most Efficient (Time)**: [Model Name]
- **Most Efficient (Tokens)**: [Model Name]
- **Best Visual Design**: [Model Name]
- **Best Documentation**: [Model Name]

### Recommendations

**For game development tasks, we recommend**:
-

**Models to watch**:
-

**Areas for improvement in benchmark**:
-

---

## ğŸ“ Appendix

### Evaluation Files
- [Model 1] Evaluation: `[path]`
- [Model 2] Evaluation: `[path]`
- [Model 3] Evaluation: `[path]`
- [Model 4] Evaluation: `[path]`

### Game Implementations
- [Model 1] Game: `[path]`
- [Model 2] Game: `[path]`
- [Model 3] Game: `[path]`
- [Model 4] Game: `[path]`

### Methodology
- Game Specification: `./prompts/game_specification.md`
- Step 1 Prompt: `./prompts/step1_planning.md`
- Step 2 Prompt: `./prompts/step2_implementation.md`
- Step 3 Prompt: `./prompts/step3_code_review.md`
